# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yiFecqIYlswU1HWfApli5LAhg2sOO_sx
"""

# -*- coding: utf-8 -*-
"""SmartSDLC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10ePKYkGkJ_HGUMLZSLNKdxBuoyzGvDYi
"""

!pip install transformers torch gradio PyPDF2

import gradio as gr
import torch
from transformers import AutoTokenizer,AutoModelForCausalLM
import PyPDF2
import io
model_name ="ibm-granite/granite-3.3-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)
if tokenizer.pad_token is None:
  tokenizer.pad_token = tokenizer.eos_token

def generate_response(prompt,max_length=1024):
  inputs = tokenizer(prompt,return_tensors="pt",truncation=True ,max_length=512)
  if torch.cuda.is_available():
    inputs = {k: v.to(model.device) for k,v in inputs.items()}

  with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_length=max_length,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id

    )
  response = tokenizer.decode(outputs[0], skip_special_tokens=True)
  response = response.replace(prompt, "").strip()
  return response

def extract_text_from_pdf(pdf_file):
  if pdf_file is None:
    return


  try:
    pdf_reader = PyPDF2.PdfReader(pdf_file)
    text = ""
    for page in pdf_reader.pages:
      text += page.extract_text() + "\n"
    return text
  except Exception as e:
    return f"Error reading PDF: {str(e)}"

def requirement_analysis(pdf_file, prompt_text):
  if pdf_file is not None:
      content = extract_text_from_pdf(pdf_file)
      analysis_prompt = f"Analysis the following document and extract key software requirement.organize them into functional requirements, non-functional requirement, and technical specification\n\n{content}"
  else:
      analysis_prompt = f"Analysis the following requriments and organizing them into functional requirements,non-functional requirements, and technical specification\n\n{prompt_text}"

  return generate_response(analysis_prompt, max_length=1200)

def code_generation(prompt, language):
  code_prompt = f"Generate {language} code for the following requirement:\n\n{prompt}\n\nCode:"
  return generate_response(code_prompt, max_length=1200)

with gr.Blocks() as app:
  gr.Markdown("# AI Code Analysis & Generator")

  with gr.Tabs():
    with gr.TabItem("Code Analysis"):
       with gr.Row():
            with gr.Column():
             pdf_upload = gr.File(label="upload PDF", file_types=[".pdf"])
             prompt_inputs = gr.Textbox(
            label="Or write requirement here",
            placeholder="Describe your software requirements....",
            lines=5
        )
            analyze_btn = gr.Button("Analyze")

    with gr.Column():
      analysis_output = gr.Textbox(label="Requirements Analysis", lines=20)

    analyze_btn.click(requirement_analysis, inputs=[pdf_upload, prompt_inputs], outputs=analysis_output)

  with gr.TabItem("Code Generation"):
    with gr.Row():
      with gr.Column():
        code_prompt = gr.Textbox(
            label="code Requirement",
            placeholder="Describe what code you want to generate...",
            lines=10
        )
        language_dropdown = gr.Dropdown(
            choices=["python", "javascript","java","c++","c#","PHP","Go","Rust"],
            label="Programming Launguage",
            value="python"
        )
        generate_btn = gr.Button("Generate Code")
    with gr.Column():
        code_output = gr.Textbox(label="Generated Code", lines=20)
        generate_btn.click(code_generation, inputs=[code_prompt, language_dropdown], outputs=code_output)
app.launch(share=True)